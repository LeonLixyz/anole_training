Processing 20 chunks
Starting evaluation for chunk 1 of 20
Starting evaluation for chunk 2 of 20
Starting evaluation for chunk 3 of 20
Starting evaluation for chunk 4 of 20
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
Geometry Reasoning: 8
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
Geometry Reasoning: 8
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_1_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
Geometry Reasoning: 8
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_2_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
check--------------------------------
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
check--------------------------------
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data

args.data_dir: /workspace/anole_training/formatted_dataargs.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json

args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.jsoncheck--------------------------------

check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
Geometry Reasoning: 8
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
Geometry Reasoning: 8
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_4_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_3_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'v_proj', 'o_proj', 'gate_proj', 'k_proj', 'q_proj', 'up_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'q_proj', 'o_proj', 'v_proj', 'up_proj', 'k_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'o_proj', 'k_proj', 'q_proj', 'gate_proj', 'down_proj', 'up_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'gate_proj', 'v_proj', 'k_proj', 'up_proj', 'q_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'gate_proj', 'q_proj', 'v_proj', 'o_proj', 'k_proj', 'down_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'q_proj', 'down_proj', 'gate_proj', 'o_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'q_proj', 'v_proj', 'down_proj', 'up_proj', 'k_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj', 'up_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'down_proj', 'o_proj', 'gate_proj', 'q_proj', 'v_proj', 'up_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'gate_proj', 'v_proj', 'up_proj', 'o_proj', 'k_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'gate_proj', 'k_proj', 'down_proj', 'v_proj', 'q_proj', 'up_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'q_proj', 'down_proj', 'gate_proj', 'o_proj', 'k_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'q_proj', 'down_proj', 'up_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'down_proj', 'up_proj', 'q_proj', 'v_proj', 'gate_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'v_proj', 'k_proj', 'gate_proj', 'q_proj', 'up_proj', 'down_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'gate_proj', 'up_proj', 'k_proj', 'down_proj', 'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'q_proj', 'up_proj', 'k_proj', 'gate_proj', 'o_proj', 'v_proj', 'down_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'down_proj', 'o_proj', 'v_proj', 'gate_proj', 'up_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'k_proj', 'v_proj', 'gate_proj', 'down_proj', 'up_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'q_proj', 'gate_proj', 'down_proj', 'v_proj', 'k_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'v_proj', 'k_proj', 'o_proj', 'q_proj', 'gate_proj', 'up_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'gate_proj', 'o_proj', 'v_proj', 'down_proj', 'k_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'v_proj', 'q_proj', 'gate_proj', 'down_proj', 'up_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'gate_proj', 'k_proj', 'q_proj', 'up_proj', 'down_proj', 'o_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'q_proj', 'gate_proj', 'down_proj', 'up_proj', 'k_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'o_proj', 'up_proj', 'q_proj', 'gate_proj', 'k_proj', 'down_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'o_proj', 'down_proj', 'gate_proj', 'k_proj', 'up_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'up_proj', 'q_proj', 'v_proj', 'o_proj', 'gate_proj', 'k_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'q_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj', 'down_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 458, Avg: 197.50
Label text - Min: 350, Max: 4954, Avg: 1811.00
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'q_proj', 'down_proj', 'o_proj', 'gate_proj', 'up_proj', 'k_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'q_proj', 'up_proj', 'k_proj', 'gate_proj', 'v_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f06544a8ef0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f06455af380>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f06544e60c0>}
generation_max_new_tokens: 16384
Loading model from /workspace/anole_training/lora-128
Trainer build successfully.
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 53, Max: 289, Avg: 183.50
Label text - Min: 355, Max: 1179, Avg: 701.38
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 237, Avg: 114.62
Label text - Min: 349, Max: 3130, Avg: 1253.50
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 237, Avg: 114.62
Label text - Min: 349, Max: 3130, Avg: 1253.50
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 237, Avg: 114.62
Label text - Min: 349, Max: 3130, Avg: 1253.50
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f90d8ee4a40>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f8de09842f0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f8ff4dc9010>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f8af55c3890>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f88ae49c080>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f88af431250>}
generation_max_new_tokens: 16384
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Trainer build successfully.
Trainer build successfully.
Loading model from /workspace/anole_training/lora-128
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fcf30d14a40>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fcf6417a3c0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fcf66535c10>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 237, Avg: 114.62
Label text - Min: 349, Max: 3130, Avg: 1253.50
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 458, Avg: 197.50
Label text - Min: 350, Max: 4954, Avg: 1811.00
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 237, Avg: 114.62
Label text - Min: 349, Max: 3130, Avg: 1253.50
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 53, Max: 289, Avg: 183.50
Label text - Min: 355, Max: 1179, Avg: 701.38
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 458, Avg: 197.50
Label text - Min: 350, Max: 4954, Avg: 1811.00
Max source length: 2048
Max target length: 5120
Trainer build successfully.
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 458, Avg: 197.50
Label text - Min: 350, Max: 4954, Avg: 1811.00
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 237, Avg: 114.62
Label text - Min: 349, Max: 3130, Avg: 1253.50
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 237, Avg: 114.62
Label text - Min: 349, Max: 3130, Avg: 1253.50
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe27c762300>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe05a7555b0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe05a7d7fe0>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 237, Avg: 114.62
Label text - Min: 349, Max: 3130, Avg: 1253.50
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 72, Max: 725, Avg: 212.25
Label text - Min: 179, Max: 1392, Avg: 607.25
Max source length: 2048
Max target length: 5120
Trainer build successfully.
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 458, Avg: 197.50
Label text - Min: 350, Max: 4954, Avg: 1811.00
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 53, Max: 289, Avg: 183.50
Label text - Min: 355, Max: 1179, Avg: 701.38
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f291b6cea20>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f2700ec6c00>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f2700e1b950>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f00641b8aa0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efd3b5d40b0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efd3ae9b1d0>}
generation_max_new_tokens: 16384
Eval Num: 24
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efb2628acc0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efb25dd39e0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efb26389040>}
generation_max_new_tokens: 16384
Raw text statistics for training data:
Input text - Min: 53, Max: 289, Avg: 183.50
Label text - Min: 355, Max: 1179, Avg: 701.38
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fcd4d0de780>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fcd4d013020>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fcd803add00>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f4d20441880>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f4d18c5ca10>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f4cc07f99d0>}
generation_max_new_tokens: 16384
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6c364cbbc0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6a1c081520>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6a243b8d10>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f3b7024f590>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f3b18a17830>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f3b702e78c0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb43f40a8d0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb43e77b7d0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb6856b28d0>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6575801910>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f632eec7740>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f632f58df70>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7c1c22b8c0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7c241beea0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7beca1ae40>}
generation_max_new_tokens: 16384
Eval Num: 24
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 72, Max: 725, Avg: 212.25
Label text - Min: 179, Max: 1392, Avg: 607.25
Max source length: 2048
Max target length: 5120
Raw text statistics for training data:
Input text - Min: 53, Max: 289, Avg: 183.50
Label text - Min: 355, Max: 1179, Avg: 701.38
Max source length: 2048
Max target length: 5120
Trainer build successfully.
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 458, Avg: 197.50
Label text - Min: 350, Max: 4954, Avg: 1811.00
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 72, Max: 725, Avg: 212.25
Label text - Min: 179, Max: 1392, Avg: 607.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 72, Max: 725, Avg: 212.25
Label text - Min: 179, Max: 1392, Avg: 607.25
Max source length: 2048
Max target length: 5120
Trainer build successfully.
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 53, Max: 289, Avg: 183.50
Label text - Min: 355, Max: 1179, Avg: 701.38
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f43077640e0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f40f41a24b0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f430796a570>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 53, Max: 289, Avg: 183.50
Label text - Min: 355, Max: 1179, Avg: 701.38
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7e46797440>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7e4695ac00>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7e463c7da0>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 53, Max: 289, Avg: 183.50
Label text - Min: 355, Max: 1179, Avg: 701.38
Max source length: 2048
Max target length: 5120
Eval Num: 24
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 72, Max: 725, Avg: 212.25
Label text - Min: 179, Max: 1392, Avg: 607.25
Max source length: 2048
Max target length: 5120
Raw text statistics for training data:
Input text - Min: 87, Max: 458, Avg: 197.50
Label text - Min: 350, Max: 4954, Avg: 1811.00
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 72, Max: 725, Avg: 212.25
Label text - Min: 179, Max: 1392, Avg: 607.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 72, Max: 725, Avg: 212.25
Label text - Min: 179, Max: 1392, Avg: 607.25
Max source length: 2048
Max target length: 5120
Trainer build successfully.
Trainer build successfully.
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 458, Avg: 197.50
Label text - Min: 350, Max: 4954, Avg: 1811.00
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 72, Max: 725, Avg: 212.25
Label text - Min: 179, Max: 1392, Avg: 607.25
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f4b083fbf80>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f4d1c92e6c0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f4ad5dfbfb0>}
generation_max_new_tokens: 16384
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f3b18292a80>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f38d167a0c0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f38d162a1e0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe2c6a34a10>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe054d553d0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe054d9e540>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f03f95386b0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f03f83b85f0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f03f8533410>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f77056326f0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f772a5aa690>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f772a56e120>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7ff6e03757c0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7ff6d89c3fe0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7ff6d903ef60>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6c723e6900>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6a004f7890>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6a5841b650>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f93c94c69c0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f93c941f440>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f93c9045d60>}
generation_max_new_tokens: 16384
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f00f083da30>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efea9754980>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efea9c77ec0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7eeddd8860>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7eedeca7b0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7cccfbbb60>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f9e8d6b14c0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f9e8d26c6b0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f9e8d4e33b0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f67b3246300>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6598563560>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f65a03c2ab0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fa658ba72f0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fa6b83b0bf0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fa6b83d6960>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f71ec6e57c0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6fa5b2c530>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6fca78ee70>}
generation_max_new_tokens: 16384
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f0029310c80>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f0028c72270>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f004e692d20>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Starting evaluation for chunk 5 of 20
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_5_of_20.json
Preparing the ['geometry_reasoning'] dataset... check--------------------------------

args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_5_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_5_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_5_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_5_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Starting evaluation for chunk 6 of 20
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_dataPreparing the ['geometry_reasoning'] dataset... 
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json

check--------------------------------args.data_dir: /workspace/anole_training/formatted_data

args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data

args.data_dir: /workspace/anole_training/formatted_dataargs.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json

args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.jsoncheck--------------------------------

check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_6_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'v_proj', 'k_proj', 'o_proj', 'down_proj', 'q_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'q_proj', 'k_proj', 'v_proj', 'down_proj', 'up_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'q_proj', 'down_proj', 'k_proj', 'o_proj', 'gate_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'q_proj', 'o_proj', 'gate_proj', 'v_proj', 'up_proj', 'down_proj', 'k_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'o_proj', 'down_proj', 'k_proj', 'q_proj', 'gate_proj', 'up_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'q_proj', 'down_proj', 'k_proj', 'v_proj', 'gate_proj', 'up_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'q_proj', 'o_proj', 'down_proj', 'up_proj', 'k_proj', 'gate_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'q_proj', 'k_proj', 'up_proj', 'down_proj', 'gate_proj', 'v_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 64, Max: 463, Avg: 141.88
Label text - Min: 570, Max: 6484, Avg: 2817.25
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f87a7564a10>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f85940475c0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f85940d59a0>}
generation_max_new_tokens: 16384
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Trainer build successfully.
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 64, Max: 463, Avg: 141.88
Label text - Min: 570, Max: 6484, Avg: 2817.25
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 64, Max: 463, Avg: 141.88
Label text - Min: 570, Max: 6484, Avg: 2817.25
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 64, Max: 463, Avg: 141.88
Label text - Min: 570, Max: 6484, Avg: 2817.25
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb5f3f539b0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb598a9f200>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb59857deb0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f0d8226cce0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f0d8219c2c0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f0db43604d0>}
generation_max_new_tokens: 16384
Trainer build successfully.
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 64, Max: 463, Avg: 141.88
Label text - Min: 570, Max: 6484, Avg: 2817.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 64, Max: 463, Avg: 141.88
Label text - Min: 570, Max: 6484, Avg: 2817.25
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe0cedba420>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe0cdd395b0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe0cedad610>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 64, Max: 463, Avg: 141.88
Label text - Min: 570, Max: 6484, Avg: 2817.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Trainer build successfully.
Raw text statistics for training data:
Input text - Min: 64, Max: 463, Avg: 141.88
Label text - Min: 570, Max: 6484, Avg: 2817.25
Max source length: 2048
Max target length: 5120
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7faee838a3f0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7faee00efb90>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7faee8377d10>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efa31488f80>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efa30942ff0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7efa31223b60>}
generation_max_new_tokens: 16384
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f83380a06b0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f80f1eb6960>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f80f2c89160>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fa29e643b60>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fa2795cae40>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fa4bfff6ab0>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Starting evaluation for chunk 7 of 20
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.jsonPreparing the ['geometry_reasoning'] dataset... 
check--------------------------------

args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
Geometry Reasoning: 8
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_7_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'v_proj', 'o_proj', 'q_proj', 'down_proj', 'gate_proj', 'k_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'up_proj', 'v_proj', 'o_proj', 'down_proj', 'q_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'gate_proj', 'k_proj', 'q_proj', 'up_proj', 'o_proj', 'down_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'down_proj', 'gate_proj', 'k_proj', 'o_proj', 'up_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'gate_proj', 'v_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'v_proj', 'k_proj', 'q_proj', 'o_proj', 'gate_proj', 'up_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'q_proj', 'gate_proj', 'down_proj', 'k_proj', 'v_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'gate_proj', 'o_proj', 'q_proj', 'up_proj', 'k_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 44, Max: 397, Avg: 197.62
Label text - Min: 357, Max: 5127, Avg: 1253.62
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 44, Max: 397, Avg: 197.62
Label text - Min: 357, Max: 5127, Avg: 1253.62
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 44, Max: 397, Avg: 197.62
Label text - Min: 357, Max: 5127, Avg: 1253.62
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 44, Max: 397, Avg: 197.62
Label text - Min: 357, Max: 5127, Avg: 1253.62
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 44, Max: 397, Avg: 197.62
Label text - Min: 357, Max: 5127, Avg: 1253.62
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 44, Max: 397, Avg: 197.62
Label text - Min: 357, Max: 5127, Avg: 1253.62
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 44, Max: 397, Avg: 197.62
Label text - Min: 357, Max: 5127, Avg: 1253.62
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 44, Max: 397, Avg: 197.62
Label text - Min: 357, Max: 5127, Avg: 1253.62
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f567aaf2b10>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f54625d3e60>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f542c95c8c0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f66894689e0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f668904f590>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f668956cbc0>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb2fdb83c50>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb0e4216660>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fb2fdcf94c0>}
generation_max_new_tokens: 16384
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f8f32970740>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f8f6436da90>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f8f32790a40>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fbdbda257c0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fbba4444530>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fbba44602c0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fc4d01a80e0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fc4d018b230>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fc49e568ec0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7faa45969490>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fa7fee13ef0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fa7ff5ace30>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f06209a8aa0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f03d9e1e0c0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f03da05c890>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Starting evaluation for chunk 8 of 20
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
check--------------------------------
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_8_of_20.json
Geometry Reasoning: 8
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'k_proj', 'gate_proj', 'up_proj', 'down_proj', 'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'o_proj', 'k_proj', 'up_proj', 'down_proj', 'gate_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'q_proj', 'gate_proj', 'up_proj', 'o_proj', 'down_proj', 'k_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'up_proj', 'v_proj', 'o_proj', 'down_proj', 'gate_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'down_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'q_proj', 'o_proj', 'down_proj', 'gate_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'q_proj', 'up_proj', 'gate_proj', 'o_proj', 'k_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 413, Avg: 150.00
Label text - Min: 179, Max: 7554, Avg: 2073.25
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fc080702810>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fbe5d763710>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fbe5da7d6a0>}
generation_max_new_tokens: 16384
Loading model from /workspace/anole_training/lora-128
Trainer build successfully.
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 413, Avg: 150.00
Label text - Min: 179, Max: 7554, Avg: 2073.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 413, Avg: 150.00
Label text - Min: 179, Max: 7554, Avg: 2073.25
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 413, Avg: 150.00
Label text - Min: 179, Max: 7554, Avg: 2073.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 413, Avg: 150.00
Label text - Min: 179, Max: 7554, Avg: 2073.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 413, Avg: 150.00
Label text - Min: 179, Max: 7554, Avg: 2073.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 413, Avg: 150.00
Label text - Min: 179, Max: 7554, Avg: 2073.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 413, Avg: 150.00
Label text - Min: 179, Max: 7554, Avg: 2073.25
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f65380fc860>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f63243020f0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f6324484a70>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fc308d471a0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fc0c2c0b080>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fc0c1cdb1d0>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f605a5b9e50>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f627c0353a0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f60354231a0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f73956d0230>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f7394b545f0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f73949fbbf0>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f225913c3e0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f2258c3a060>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f249fcc20f0>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f196c9c56a0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f19cc2bd400>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f19cc2a9070>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe03199b680>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe2784c28a0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe0314276e0>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Starting evaluation for chunk 9 of 20
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... Preparing the ['geometry_reasoning'] dataset... 

args.data_dir: /workspace/anole_training/formatted_data
args.data_dir: /workspace/anole_training/formatted_dataargs.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json

check--------------------------------args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json

check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_9_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'down_proj', 'q_proj', 'gate_proj', 'v_proj', 'o_proj', 'k_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'gate_proj', 'v_proj', 'q_proj', 'down_proj', 'k_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'q_proj', 'o_proj', 'gate_proj', 'down_proj', 'k_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'gate_proj', 'up_proj', 'o_proj', 'q_proj', 'down_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'up_proj', 'q_proj', 'v_proj', 'gate_proj', 'down_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'v_proj', 'q_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'gate_proj', 'down_proj', 'up_proj', 'o_proj', 'q_proj', 'k_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k_proj', 'up_proj', 'o_proj', 'q_proj', 'gate_proj', 'v_proj', 'down_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 480, Avg: 231.00
Label text - Min: 495, Max: 1884, Avg: 1071.75
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 480, Avg: 231.00
Label text - Min: 495, Max: 1884, Avg: 1071.75
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fd9d5b14e90>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fd9d57ad8e0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fd9d6293680>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 480, Avg: 231.00
Label text - Min: 495, Max: 1884, Avg: 1071.75
Max source length: 2048
Max target length: 5120
Trainer build successfully.
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 480, Avg: 231.00
Label text - Min: 495, Max: 1884, Avg: 1071.75
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 480, Avg: 231.00
Label text - Min: 495, Max: 1884, Avg: 1071.75
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f81043955b0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f810c202f30>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f831f061490>}
generation_max_new_tokens: 16384
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f838d322210>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f838d56ea20>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f81469abb60>}
generation_max_new_tokens: 16384
Trainer build successfully.
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 480, Avg: 231.00
Label text - Min: 495, Max: 1884, Avg: 1071.75
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 480, Avg: 231.00
Label text - Min: 495, Max: 1884, Avg: 1071.75
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f268e5745c0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f268e55ea80>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f268e5a4a70>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fea64ddcb30>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe81efbb830>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe81decf440>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 87, Max: 480, Avg: 231.00
Label text - Min: 495, Max: 1884, Avg: 1071.75
Max source length: 2048
Max target length: 5120
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f0f36f3aed0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f0d242b4230>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f0f36f0ec90>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f5d32d8b0b0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f5d643701a0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f5d32fe3b60>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7feca1bf3bc0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fecc653cd40>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7feee83d4860>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
********************
24
24
********************
Starting evaluation for chunk 10 of 20
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
check--------------------------------
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
Geometry Reasoning: 8
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
check--------------------------------
Preparing the ['geometry_reasoning'] dataset... 
args.data_dir: /workspace/anole_training/formatted_data
args.custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
check--------------------------------
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
data dir: /workspace/anole_training/formatted_data
custom_dataset_path: /workspace/anole_training/formatted_data/chunk_10_of_20.json
Geometry Reasoning: 8
Available splits in dataset: ['train', 'validation', 'test']
data: {'train': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'validation': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
}), 'test': Dataset({
    features: ['input_text', 'input_img_paths', 'label_text', 'label_img_paths', 'task', 'train_task', 'idx'],
    num_rows: 8
})}
Adding train split with 8 examples
Adding validation split with 8 examples
Adding test split with 8 examples
Created merged evaluation dataset with 24 examples
model_ckpt_path: /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v_proj', 'o_proj', 'q_proj', 'down_proj', 'up_proj', 'gate_proj', 'k_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'o_proj', 'k_proj', 'q_proj', 'v_proj', 'down_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'k_proj', 'gate_proj', 'q_proj', 'down_proj', 'v_proj', 'o_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'gate_proj', 'k_proj', 'o_proj', 'up_proj', 'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'up_proj', 'o_proj', 'gate_proj', 'k_proj', 'q_proj', 'down_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'down_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'up_proj', 'v_proj', 'gate_proj', 'k_proj', 'down_proj', 'q_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 215, Avg: 108.88
Label text - Min: 432, Max: 6062, Avg: 1867.25
Max source length: 2048
Max target length: 5120
config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o_proj', 'gate_proj', 'up_proj', 'q_proj', 'k_proj', 'down_proj', 'v_proj'}, exclude_modules=None, lora_alpha=256, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 215, Avg: 108.88
Label text - Min: 432, Max: 6062, Avg: 1867.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 215, Avg: 108.88
Label text - Min: 432, Max: 6062, Avg: 1867.25
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f242df9f590>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f242ed8ef90>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f242dd13800>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 215, Avg: 108.88
Label text - Min: 432, Max: 6062, Avg: 1867.25
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
Loading model from /workspace/anole_training/lora-128
Trainer build successfully.
Loading model from /workspace/anole_training/lora-128
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 215, Avg: 108.88
Label text - Min: 432, Max: 6062, Avg: 1867.25
Max source length: 2048
Max target length: 5120
Loading model from /workspace/anole_training/lora-128
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f3a122d89b0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f3a12fc48c0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f3a1248d220>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f58344daba0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f580121cc50>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f582606bb60>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f99d4278b00>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f99c52237d0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f99c5186d20>}
generation_max_new_tokens: 16384
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 215, Avg: 108.88
Label text - Min: 432, Max: 6062, Avg: 1867.25
Max source length: 2048
Max target length: 5120
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 215, Avg: 108.88
Label text - Min: 432, Max: 6062, Avg: 1867.25
Max source length: 2048
Max target length: 5120
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Eval Num: 24
Raw text statistics for training data:
Input text - Min: 52, Max: 215, Avg: 108.88
Label text - Min: 432, Max: 6062, Avg: 1867.25
Max source length: 2048
Max target length: 5120
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f1048494110>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f1018aad490>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f10189c6960>}
generation_max_new_tokens: 16384
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe720deaed0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe720deb230>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fe4d9e9b710>}
generation_max_new_tokens: 16384
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f8436f6fb00>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f843665cec0>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7f8436523b60>}
generation_max_new_tokens: 16384
Trainer build successfully.
Trainer build successfully.
tokenized_data: {'train': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fdfb0b557c0>, 'eval': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fdfb0364890>, 'test': <utils.interleaved_tokenized_dataset.InterleaveAnoleTokenizedDataset object at 0x7fdfb0ad47a0>}
generation_max_new_tokens: 16384
Trainer build successfully.
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/0_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/0_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/1_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/1_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/2_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/2_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/6_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/6_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/7_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/7_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/2_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/2_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/7_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/7_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/6_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/6_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/1_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/1_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/0_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/0_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/3_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_1_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_2_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/4_3_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/2_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/2_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/7_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/7_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/6_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/6_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/1_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/1_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/5_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/0_0_eval_None.png
saved sketch to outputs/vlm_reasoning_eval_chunk_4_of_20/vlm_reasoning_eval_chunk_4image_seq_len-1024-anole-hyper-train1val1lr3e-05-geometry_reasoning-prompt_anole-42/sketch_eval_None/0_0_eval_None.png
***** eval metrics *****
  eval_runtime            = 0:21:23.10
  eval_samples            =         24
  eval_samples_per_second =      0.019
